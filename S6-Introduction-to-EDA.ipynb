{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f70af6e4-e73a-4c61-84e1-9a549de207fa",
   "metadata": {},
   "source": [
    "# Introduction to EDA\n",
    "\n",
    "## Course outline :\n",
    "\n",
    "1. S6 (introduction) : what is EDA, pandas methods for data cleansing\n",
    "2. S7 DataViz : matplotlib, seaborn, plotly, how to deal with outliers\n",
    "3. S8 EDA recap, exercise\n",
    "4. S9 correction\n",
    "5. S10 regressions (linear, logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd8c779-b343-4bd5-bb5a-599e067d1257",
   "metadata": {},
   "source": [
    "## What is EDA\n",
    "\n",
    "Presentation of EDA on [wikipedia.org](https://en.wikipedia.org/wiki/Exploratory_data_analysis) :\n",
    "```\n",
    "In statistics, exploratory data analysis (EDA) is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell beyond the formal modeling and thereby contrasts with traditional hypothesis testing, in which a model is supposed to be selected before the data is seen. \n",
    "```\n",
    "\n",
    "I would define data analysis as : collecting, transforming, and organizing data to formulate hypotheses, draw conclusions, make predictions, and support decision-making based on statistically proven facts.\n",
    "\n",
    "![Data analysis cycle](./images/AnalysisCycle.png)\n",
    "\n",
    "Typical data science workflow :\n",
    "\n",
    "1. Get raw data (.csv, API/JSON, BDD/SQL,…)\n",
    "2. Browse the data, read the columns/variables and make a working copy\n",
    "3. Descriptive statistics: mean, median, max, min, standard deviation, quantiles, variance, covariances, correlation…\n",
    "4. Cleansing: manage NaNs (delete? replace with mean, median?), convert other problematic values – such as inf and -inf–, delete columns (useless or unusable columns)\n",
    "5. If necessary, assemble the data (merge), create the metrics we are missing (create new columns, transform…)\n",
    "6. Visualize the data (distribution, linking) with graphs adapted to the data types (bar plots, scatter plots, cat plots, box plots…)\n",
    "7. Identify and manage outliers (delete or transform)\n",
    "8. Choose, train and test ML or analysis models (statistical inference)\n",
    "9. Improve models (fine-tuning, feature selection) and comparison of best-fit models (A/B testing)\n",
    "10. Deployment : forecasting with ML model, or dashboard for data storytelling / decision making)\n",
    "\n",
    "The primary goal of EDA (Exploratory Data Analysis) is to build a good representation and, above all, *understanding* of the data by identifying how observations and variables are related to each other, highlighting trends and detecting anomalies. This is the first essential step, whatever is our objectives,  whether creating a complex machine learning model or simply testing the data to make a decision. It is from this exploration that you will formulate the hypotheses (on the relationships between variables) that will guide your work and that you will test.\n",
    "\n",
    "From an operational point of view, you will :\n",
    "\n",
    "- establishing basic statistics that can help us describe the data and observed trends\n",
    "- generating relevant dataviz\n",
    "- prepare the data for the upcoming analyses\n",
    "- in particular, data cleansing : identifying errors, outliers and missing values, chose how to process them\n",
    "\n",
    "A lot of tools can be relevant : \n",
    "\n",
    "![Tools](./images/Tools.png)\n",
    "\n",
    "In this course we will focus on : \n",
    "\n",
    "* `pandas` and `numpy` for data cleansing and preparation \n",
    "\n",
    "* `matplotlib`, `seaborn`, `plotly` for dataviz\n",
    "\n",
    "* `statsmodels` for simple analysis (regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21d662-053d-411e-8ce2-e0e1adc67e97",
   "metadata": {},
   "source": [
    "## `pandas` and `numpy` methods for data exploration and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a5e73-10a4-4869-9758-578cb1bd5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02e286-3a49-4e82-89a0-2cb369a000ba",
   "metadata": {},
   "source": [
    "### Load and first look on data :\n",
    "\n",
    "First, let’s import some data. The `seaborn` library has some training dataset. Let’s load a dataset and have a first look on it with the `.head()` method :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d8c2e-083c-46fc-b1a5-0c1878d31623",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df = sns.load_dataset('tips')\n",
    "tips_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f663c3e-53ff-455b-9811-0f0b5ac5f758",
   "metadata": {},
   "source": [
    "A handful of methods of the `pandas` library are absolutely essential because they will allow us to carry out most of data exploration and cleaning. You have to know these methods to acquire an initial understanding of the data.\n",
    "\n",
    "If you are not fluent with `pandas`, I recommand you to follow this [excellent notebook](https://github.com/virgilus/data-science/blob/main/Pandas_Introduction.ipynb) of my colleague [Virgile Pesce](https://github.com/virgilus). Especially if you need a refresher about methods like `.loc`/`.iloc`, `.groupby()`, `.merge()`, etc.\n",
    "\n",
    "* `df.info()`: the first method to call once you have loaded your data in the form of a DataFrame. This method allows you to have the following information at once:\n",
    "\n",
    "    * the size (*shape*) of the table (rows / columns)\n",
    "    * the name of each column (name of *a priori* variables)\n",
    "    * the number of non-null values in each column\n",
    "    * the datatype of the values in each column\n",
    "    * the size in memory of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85e2877-7349-48fd-92b3-6486d29a62cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3512dac7-42eb-494d-af20-b0e6774208fb",
   "metadata": {},
   "source": [
    "* for numeric/quantitative data: `df.describe()`. This method will provide you with the basic descriptive statistics (count, mean, standard deviation, median, min, max, quartiles, etc.) allowing you to represent the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee1a12-4551-4f9a-886c-8584d15a6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a520b89-0d78-4555-aa2d-e28c2c6f6cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(tips_df.describe(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21121b65-5f1f-41f6-88ab-d5a13124a70c",
   "metadata": {},
   "source": [
    "* For categorical/qualitative variables: `df['column'].value_counts()`. This function allows you to quickly count the number of elements in each category (which are then enumerated at the same time). This advantageously replaces the use of the `df.unique()` and `df.nunique()` methods which would need more steps to give same information :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf38b434-ece4-465d-9d86-50876a761b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df['day'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab13e0-8375-4e8c-807b-8865a539147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_wna_df = tips_df.copy()\n",
    "tips_wna_df.iloc[1] = float('nan')\n",
    "tips_wna_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340adc0-d7f0-41b2-8542-b845d62bb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_wna_df['day'].value_counts(dropna=False, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8f78ab-edf5-4d9d-b950-13634df245f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(tips_df['day'].value_counts(normalize=True),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22762bf5-b44e-4417-bd24-222b5b928c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6113724-416a-4c5d-8183-01835309797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df['day'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e62150b-12af-41a7-91f9-566d6d919140",
   "metadata": {},
   "source": [
    "* a useful statistics is the mode `df.mode()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773a3a5-11fa-4b7e-963a-63a5604a7e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df['day'].mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098256a6-7867-4cfa-bbe2-1d92872d82b5",
   "metadata": {},
   "source": [
    "If you want to get the mode for each columns, you have to use a *lambda function* with `.apply()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4704847-70e7-44b9-9ec5-4c07ef45a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.apply(lambda x: x.mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acfc33b-803e-4b6d-a21f-4083bda4060c",
   "metadata": {},
   "source": [
    "Lambda functions or anonymous functions are often used to implement operations that we will only perform once but on a certain number of elements. We will therefore declare a function on the fly that we will never call again afterwards. We could also have written :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45bd7b7-36c5-48ba-aad6-cf50c60553f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mode(x):\n",
    "    return x.mode()\n",
    "\n",
    "tips_df.apply(get_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b3772e-e0ac-483a-803b-6a6dce1c99c1",
   "metadata": {},
   "source": [
    "That’s how we write custom treatment for data preparation !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa499a-0488-4b62-92d9-8d67063c32ce",
   "metadata": {},
   "source": [
    "### Cleansing\n",
    "\n",
    "Now that we have had a first look on data, knowing columns names, datatypes and eventually if values are missing, we can do some cleansing.\n",
    "It’s time to correct some issues :\n",
    "\n",
    "* are the columns correctly named ?\n",
    "* datatypes issues (generally its `datetime` issues : dates are in `string` format and not a `timestamp` type)\n",
    "* sometime we may change the index, drop columns… (+ other time operation like periodicity)\n",
    "* is there duplicated rows ?\n",
    "* how to deal with missing values ?\n",
    "\n",
    "#### Formating : rename columns\n",
    "\n",
    "The method to rename columns is `.rename()`, which take dictionnary to associate old_name /new_name for columns. For example, if we find that `time` is not a very meaningful name, and want to rename this column `meal` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef776f18-2da5-4df8-93ff-8011dfd3e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.rename(columns={'time': 'meal'}, inplace=True)\n",
    "tips_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197c965-ad1d-4702-893a-ec65319dad87",
   "metadata": {},
   "source": [
    "We may want to process all columns at once, for example by wanting to standardize naming formats :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1463ddd-e841-48a6-a777-839a3aba6bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tips_df.columns:\n",
    "    tips_df.rename(columns={c: c.upper()}, inplace=True)\n",
    "tips_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9a7e2c-5176-4389-a969-82618eb4a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tips_df.columns:\n",
    "    tips_df.rename(columns={c: c.lower()}, inplace=True)\n",
    "tips_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e034d0cf-c1ac-48f5-8def-2fbbe9393c9e",
   "metadata": {},
   "source": [
    "#### Datatypes : date issue\n",
    "\n",
    "Date formats and the datatype of variables that hold dates often cause problems with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ab93f-6d24-4be3-ac0d-4f053d9cf849",
   "metadata": {},
   "outputs": [],
   "source": [
    "dj_df = sns.load_dataset('dowjones')\n",
    "dj_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21f4b17-10df-425e-8245-24045d25990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3680404c-a08b-4dd2-be39-84645b451e48",
   "metadata": {},
   "source": [
    "If you download real data that countains datetime values (for example [this dataset on Kaggle](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5f025-81cc-44de-94f0-22a59ab275f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = pd.read_csv('data/olist-dataset/olist_orders_dataset.csv')\n",
    "orders_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28df4e-f606-4399-a951-a72260bef0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5236f-3799-4c76-8332-1995b0b36ab6",
   "metadata": {},
   "source": [
    "You notice that all date variable, (`order_purchase_timestamp`, `order_delivered_carrier_date`…) are of the `object` type (in fact, `string`) and not of the `timestamp` type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8431dee-68fe-479a-92e4-582636e4e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df['order_purchase_timestamp'] = pd.to_datetime(\n",
    "    orders_df['order_purchase_timestamp'])\n",
    "\n",
    "orders_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb792bfc-87ec-46f0-b026-8aefd50ed37a",
   "metadata": {},
   "source": [
    "#### Index and dropping useless columns\n",
    "\n",
    "One of the specificities of pandas is the existence of an index for rows which can be of any type. It can be interesting to identify one record by its timestamp.\n",
    "\n",
    "* `.set_index()` method allows us to use a column as index :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1084c2b2-d226-4b87-8cf8-f4fa9ecf7391",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = orders_df.set_index(\n",
    "    'order_purchase_timestamp').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05942663-361f-46ac-b8f0-de096b9105eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d596efd-fd7f-435a-afc8-99c9fd5f3f7f",
   "metadata": {},
   "source": [
    "* `.drop()` method gets rid of a column or a row :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e1df8-9ba8-41c8-8df2-015b2cf0955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.drop(['order_id'], axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759c207-167a-419e-b25e-3a808ff0156f",
   "metadata": {},
   "source": [
    "Don’t forget the flag `inplace` if you want to permanently modify the dataframe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ca628-8aec-4062-bbda-09dcc297f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1d3e4a-5eba-403f-84ef-a096efbc33da",
   "metadata": {},
   "source": [
    "Another formalism to drop columns is to use use the `columns` argument :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55900272-8bd7-4294-a8f1-d9a894c258d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders_df.drop(columns=['order_id', 'customer_id'], inplace=True)\n",
    "orders_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d761df-ffb6-4cbb-96c5-1387f6e395f0",
   "metadata": {},
   "source": [
    "You can do the same with index (or `axis=0`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1039556-c181-411f-9527-824cfb1cf317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "orders_df.drop(index=datetime.datetime(2016, 9, 4, 21, 15, 19)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1040e8f-bb2b-4473-b0fe-bebf190fe9c3",
   "metadata": {},
   "source": [
    "We can perform complex exploration on the basis of datetime values, for example here we count the number of orders delivered weekly during year 2017 according to the carrier timestamp :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b053c-4a86-43c8-a0a8-c68ed7e75d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivered_weekly = (orders_df.sort_index()\n",
    "     .query(\"order_status != 'delivered'\")\n",
    "     .loc['2017-01-01':'2017-12-31']\n",
    "     .resample('W')['order_delivered_carrier_date']\n",
    "     .count())\n",
    "delivered_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b9575-387e-42fe-b8f7-c5a3fffa15b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "delivered_weekly.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da204ad-fbd5-4b81-a60c-428c4014c495",
   "metadata": {},
   "source": [
    "#### Duplicated rows ?\n",
    "\n",
    "Raw datafiles may contain a lot of errors, one of them being the duplication of records, for any reason.\n",
    "\n",
    "* `.duplicated()` checks if there are duplicated rows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02302b45-be59-4f04-aada-c5d73c722963",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ac721-26d0-4a9a-9022-f3bb5e7daa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df[tips_df.duplicated(keep=False)] # keep can be'first' or 'last'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10766be3-e441-46d7-b4fb-8cf52bd3794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf00f9-e065-47ae-b76e-308eeb92222a",
   "metadata": {},
   "source": [
    "* `.drop_duplicates()` get rid of duclicate rows, `keep` being at `'first'` by default (don’t forget `inplace`) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b104298f-61d4-4da6-911c-92cba6e4c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_cleaned = tips_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b2fed-4ce9-42f5-b5ca-d17c182e7663",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fa8fb1-2039-4bb4-8d97-3729ac768a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf51a3-0d55-4c01-ae87-c55ea0d1333f",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "\n",
    "Missing values can have very negative effects on our analyses (poor estimation of the distribution, poor estimation of the parameters of a model, effects on the variance, etc.) like any other data defect, but also two specific negative effects:\n",
    "\n",
    "- often missing values obey a pattern (faulty data collection, accident, difficulties in a place, a date, a target, in particular). If this pattern is not explained, it can contaminate the model or the analyses which will poorly reflect reality (statistical bias by not capturing an effect that nevertheless exists)\n",
    "\n",
    "- if data is missing, it is as if our sampling was not complete, and therefore undermines our ability to highlight or identify significant relationships between variables in our analyses (loss of statistical power)\n",
    "\n",
    "#### A typololgy of missing values\n",
    "\n",
    "1. values that are missing completely randomly (example of cause : incident during the collection or copy of data)\n",
    "\n",
    "2. values that are missing randomly (any value can be missing), but depending on the value of another variable, which is known (people with low level of education tend to ignore some questions, but not always)\n",
    "\n",
    "3. values that are missing non-randomly (these are just given values that can be missing, for example we cannot record values greater than 10 and this variable had values greater than 10)\n",
    "\n",
    "4. missing values for questions (often in questionnaires), measurements, or even specific variables within observations\n",
    "entire records that are missing (not just a few variables, but a complete line), for example a questionnaire that is returned empty\n",
    "\n",
    "We note that the context and the type of missing values will be very important in choosing the appropriate treatment. Moreover, for several cases, at this level of the course, we do not have the necessary knowledge to implement certain solutions. They are presented here just for documentation purposes. Come back to them when you are better equipped in terms of statistics and modeling.\n",
    "\n",
    "#### Dealing with missing values\n",
    "\n",
    "There are two strategies: deletion or replacement.\n",
    "\n",
    "1. delete any record (row) where a value is missing, with the `df.dropna()` method. A radical and simple solution, but which risks to drastically reduce the sample size\n",
    "\n",
    "2. for totally random missing values (case (1) in the list above), a good technique is to replace the missing value with the mean or mode (the most frequent value) of the variable concerned. This is called *imputation*. This can be done manually (define a function that replaces the missing values with, at choice, the mean or the mode), but the Scikit-Learn machine learning library has a whole set of methods for this – sklearn.inpute module. You will see this in you take a machine learning module, for the moment we will do these replacements “by hand”. This technique is problematic however in situations where the missing values are linked to another variable (cases (2) and (3) above).\n",
    "\n",
    "3. If we suspect a pattern behind the missing values, we can consider creating a model that will allow, from the values taken by the other observed variables, to recreate the supposed values of the missing variable. We can use a regression model, a correlation model, or models that reconstruct the distribution of the missing value. Clustering models can also intervene (by identifying other records that \"resemble\" the one where the value is absent, but where it is present on the other hand and where we will copy it). There are other more complex models adapted to this objective: *multivariate imputation through chained equations*, *pattern mixture model*, *predictive mean matching*, etc. of course they are out of the scope of this introduction course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1764de9-fe25-4735-ac8b-1a527eabeb33",
   "metadata": {},
   "source": [
    "* `.info()` method indicates count of non-null values in each columns. If yoou see a difference in the count between columns, this is the sign that there are missing values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809864d-c1f1-4ce9-af93-bc71accb9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf0ac61-7008-4325-8dd9-d5247fe9f2b8",
   "metadata": {},
   "source": [
    "* `.isna()` method gives the rows where a missing value is found :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fe4b93-3630-4d2a-9673-bc6747db38af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61332538-e5d0-49f5-b141-7706d661712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.isna().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b3eb1b-beb2-48b5-af40-8194fda9e867",
   "metadata": {},
   "source": [
    "* we can find out the count of missing value in each column by chaining with the `.sum()` method :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2929b0-df3d-4a8d-bdb5-26bc6016a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e0ee6d-697e-4dd8-a99d-023bbfe07fa9",
   "metadata": {},
   "source": [
    "* to do deeper investigation (e.g. : to see if there is a pattern) we can give a look at the rows containing missing values on a specific column with boolean indexing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fbc66b-1e54-4d95-b56f-d0ef18ecb535",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df[orders_df['order_approved_at'].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8cd904-a3fe-4fd5-941e-32f6f1ea4f8f",
   "metadata": {},
   "source": [
    "* `.dropna()` as its name suggests, drop rows with missing value. You can define :\n",
    "    * a `subset` argument to select specific columns,\n",
    "    * an `axis` too (drop columns or rows that countains missing values),\n",
    "    * `how` defines the behavior of `.dropna` : if `how='any'` (default) a row or a column will be dropped if it contains at least one missing value, if `how='all'`, it will be dropped only if all values are missing (empty column or row)\n",
    "    * `inplace` if we want to permanently modify the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d7309-bbbd-4cf0-a4fc-553983ee9532",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f107c928-9a74-40b5-99a0-cf66604ecf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964e1949-aef2-443c-87ba-5daa0aacb2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.dropna(subset=['order_approved_at']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58922c7-d0b2-47ec-81a2-8118b55c3cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.dropna(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53372a9b-62c1-44b8-a209-16682b74eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.dropna(axis=1, how='all').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e83c7f-f9c2-4424-9710-f1fd0c275da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.dropna(axis=0, how='all').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b163b51-dda6-4833-bf8d-afb03bd1fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_wna_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8108e1-fa8d-474d-91ea-ef00e6c51a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_wna_df.dropna(how='all').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5c24b-faeb-482a-a1a0-b7d6a25467d0",
   "metadata": {},
   "source": [
    "Try to write a function that replace the missing values in a given column by the mode of this column.\n",
    "\n",
    "Test it by replacing the missing values in `tips_wna_df` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf3c79a-98ef-4913-924f-8d2316500354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_NaN(df, column):\n",
    "\n",
    "    # your code here\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecfcc72-c709-40c5-a8b6-706d23380d42",
   "metadata": {},
   "source": [
    "For this introductory course, we will limit ourselves to the first two solutions (drop rows with missing values, or replace the missing values by the mean or mode). \n",
    "\n",
    "Some advice: first, look for whether the absence of a value obeys a pattern, the most favorable situation being that these absences are totally random. Then ask yourself whether the affected variable is important for our analysis or not (no point in wasting time for nothing, with a poor quality measurement). This is where business knowledge, or domain knowledge is important. \n",
    "\n",
    "IMPORTANT : `.isna()`, `.dropna()` methods deal with the `NaN` datatypes. Sometimes missing values, errors, etc. are coded differently. For example if a column gives altitude, some points with errors of measurement may have a fixed negative \"altitude\" value (for example `-100000`). You have to inspect if such encoding is used in your data and find a way to deal with it. \n",
    "\n",
    "Keep a good record of all the manipulations you perform You will probably have to apply different methods: elimination in some, replacement by averages, ditto for outliers, etc. Cleaning is also an iterative process, it is extremely important to make your processing reproducible and traceable (know what you did, in what order, etc.). So never work directly on raw data (make copies !!!) and keep logs !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f759420c-412f-4f45-a585-f4402eb4cb86",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df03763-27fc-42da-bda3-086c3a4e031d",
   "metadata": {},
   "source": [
    "To go further, we will need to do some dataviz which is the subject of the next course.\n",
    "\n",
    "Right now you need to practice manipulating dataframes and the pandas methods that allow you to explore and clean them.\n",
    "\n",
    "Here are some datasets that are freely available (downloadable), and that can be used as support to implement EDA techniques:\n",
    "\n",
    "1. Synthesize the data with descriptive statistics\n",
    "2. Identify if these data need to be cleaned (missing values? type problem?)\n",
    "3. Make some simple `.plot()` or `.corr()` to see if some pattern emerges\n",
    "\n",
    "Note : do not necessarally\n",
    "\n",
    "### House prices\n",
    "\n",
    "Sale price of real estate and many variables:\n",
    "[house prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "This dataset is proposed as part of a machine learning topic, we will not go that far. We will just explore this data. Also, only three files interest us: train.csv and test.csv that will need to be assembled into a single dataframe (`.concatenate()`), and the text file that accompanies it to fully understand the data, which will allow you to formulate hypotheses to guide your exploration.\n",
    "\n",
    "### Loan predications\n",
    "\n",
    "Here is an interesting dataset to study the question of outliers that we will study later. For now, just explore and clean the dataset [loan predication](https://www.kaggle.com/datasets/ninzaami/loan-predication/data)\n",
    "\n",
    "The problem is to relate characteristics of individuals, and the susceptibility to grant them a loan.\n",
    "\n",
    "### Oil and gas US production \n",
    "\n",
    "his dataset shows US oil and gas production from June 2008 to June 2018: [us-oil-and-gas-production](https://www.kaggle.com/datasets/djzurawski/us-oil-and-gas-production-june-2008-to-june-2018)\n",
    "\n",
    "This data includes temporal information.\n",
    "\n",
    "In addition to classic explorations, this dataset is of interest:\n",
    "\n",
    "* you will need to convert objects (string) into timestamps to be able to exploit this data\n",
    "* with ```.groupby()``` try to calculate and represent annual gas and oil production (periodicity)\n",
    "* try also to present production by state\n",
    "* assemble into a single synthetic dataframe, by year, gas **and** oil production, for the whole of the US only, and another, state by state.\n",
    "\n",
    "### Cars CO2 emissions\n",
    "\n",
    "This dataset lists the amount of CO2 emitted for vehicles of different models: [vehicle-co2-emissions-dataset](https://www.kaggle.com/datasets/brsahan/vehicle-co2-emissions-dataset/data)\n",
    "\n",
    "The opportunity to make some very informative visualizations! (next course)\n",
    "\n",
    "### Real e-shop data\n",
    "\n",
    "If you find the previous datasets too simple and do not require enough cleaning work, etc., here is a (real) dataset that will give you hard time: [Olist dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)\n",
    "\n",
    "A good exercise to learn how to merge data, and create metrics.\n",
    "\n",
    "1. Observe the database schema carefully\n",
    "2. From there, try to create a single dataframe (you will have to do ```.merge(), .groupby()```, etc.) that contains all the interesting data, according to hypotheses that you can make: for example, is there a correlation between the speed at which a consumer receives a product and the appreciation he leaves for a seller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2debf958-f743-4e98-8e96-0b4f578f7458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
